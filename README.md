# AI4Bharat Real-time Speech-to-Text API

A production-ready FastAPI backend for real-time speech-to-text transcription of Indic languages with support for both ONNX and HuggingFace transformer models.

## ğŸŒŸ Features

- **Multi-language Support**: 11 Indic languages (Hindi, Bengali, Tamil, Telugu, Marathi, Gujarati, Kannada, Malayalam, Punjabi, Odia, Assamese)
- **Dual Model Support**: ONNX models for production + HuggingFace transformers as fallback
- **Real-time Processing**: Low-latency transcription with performance metrics
- **Batch Processing**: Transcribe multiple files in a single request
- **Comprehensive API**: RESTful endpoints with OpenAPI documentation
- **Robust Error Handling**: Graceful fallbacks and detailed error reporting
- **Performance Monitoring**: Real-time factor (RTF) and confidence scoring

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Start the Server

```bash
python start_server.py
```

Or manually:

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

### 3. Access the API

- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health
- **Root Endpoint**: http://localhost:8000/

## ğŸ“¡ API Endpoints

### Transcribe Audio

```http
POST /api/v1/transcribe
Content-Type: multipart/form-data

audio: <audio_file>
language: hi|bn|ta|te|mr|gu|kn|ml|pa|or|as (default: hi)
decoding: ctc|rnnt (default: ctc)
normalize: true|false (default: false)
```

### Batch Transcription

```http
POST /api/v1/transcribe/batch
Content-Type: multipart/form-data

files: [<audio_file1>, <audio_file2>, ...]
language: hi|bn|ta|te|mr|gu|kn|ml|pa|or|as (default: hi)
decoding: ctc|rnnt (default: ctc)
```

### Get Supported Languages

```http
GET /api/v1/languages
```

### Get Performance Statistics

```http
GET /api/v1/stats
```

## ğŸ§ª Testing

Run the test suite:

```bash
python test_backend.py
```

## ğŸ“ Project Structure

```
REALTIME_STT/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes.py          # API endpoints
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ ai4bharat_stt.py   # STT engine with dual model support
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ infer.py           # Legacy inference code
â”‚   â”‚   â””â”€â”€ stt_service.py     # Alternative STT service
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ audio.py           # Audio processing utilities
â”‚   â”‚   â””â”€â”€ text.py            # Text processing utilities
â”‚   â”œâ”€â”€ config.py              # Application configuration
â”‚   â””â”€â”€ main.py                # FastAPI application
â”œâ”€â”€ indic_conformer_model/     # ONNX model directory (optional)
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ start_server.py            # Startup script
â”œâ”€â”€ test_backend.py            # Test suite
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

Key configuration options in `app/config.py`:

- `SUPPORTED_LANGUAGES`: List of supported language codes
- `DECODING_MODES`: Available decoding modes (CTC, RNN-T)
- `MAX_FILE_SIZE`: Maximum upload size (50MB default)
- `ALLOWED_EXTENSIONS`: Supported audio formats
- `USE_GPU`: Enable/disable GPU usage

## ğŸ”§ Model Support

### ONNX Model (Primary)

Place your ONNX model files in `indic_conformer_model/` directory with:
- `model_onnx.py`
- Required ONNX model files

### Transformers Model (Fallback)

Automatically uses `ai4bharat/indic-conformer-600m-multilingual` from HuggingFace when ONNX model is not available.

## ğŸ“Š Performance Metrics

The API returns detailed performance information:

- **Text**: Transcribed text
- **Confidence**: Confidence score (0-100%)
- **Inference Time**: Processing time in seconds
- **RTF**: Real Time Factor (inference_time / audio_duration)
- **Audio Duration**: Length of input audio
- **Model Type**: Which model was used (onnx/transformers)

## ğŸ› ï¸ Development

### Running Tests

```bash
python -m pytest tests/ -v
```

### Code Quality

```bash
flake8 app/
black app/
mypy app/
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- [AI4Bharat](https://ai4bharat.org/) for the Indic language models
- FastAPI for the web framework
- HuggingFace for transformer models
=======
# ğŸ™ï¸ Unified STT Service
## Production-Grade Speech-to-Text for English & 22+ Indian Languages

A high-performance, scalable speech-to-text service supporting:
- **English**: OpenAI Whisper (faster-whisper)
- **Indian Languages**: AI4Bharat Indic Conformer (22+ languages)
- **Hinglish**: Automatic mix-language support

---

## ğŸ“‹ Features

âœ… **Multi-Language Support**
- English (via Whisper)
- 22+ Indian Languages: Hindi, Bengali, Tamil, Telugu, Marathi, Gujarati, Kannada, Malayalam, Punjabi, Urdu, Assamese, Odia, Sanskrit, Sindhi, Nepali, Sinhala, Konkani, Maithili, Dogri, Manipuri, Bodo, Santali

âœ… **Intelligent Routing**
- Automatic language detection
- Hinglish (Hindi-English mix) support
- Optimized model selection per language

âœ… **Production Ready**
- Structured logging (JSON format)
- Comprehensive error handling
- Graceful shutdown
- Request tracking
- Background task cleanup

âœ… **Performance Metrics**
- Real-Time Factor (RTF) calculation
- Confidence scoring
- Word Error Rate (WER) if ground truth provided
- Accuracy reporting

âœ… **Batch Processing**
- Multi-file transcription
- Parallel processing support
- Error recovery per file

---

## ğŸ—ï¸ Project Structure

```
REALTIME_STT/
â”œâ”€â”€ models/                          # STT Model implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                     # Abstract base class
â”‚   â”œâ”€â”€ ai4bharat_stt.py           # AI4Bharat implementation
â”‚   â””â”€â”€ whisper_stt.py             # Whisper implementation
â”‚
â”œâ”€â”€ services/                        # Business logic layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ router.py                  # Unified routing logic
â”‚   â”œâ”€â”€ audio_utils.py             # Audio processing utilities
â”‚   â””â”€â”€ metrics.py                 # Metrics calculation
â”‚
â”œâ”€â”€ utils/                           # Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py                  # Structured logging
â”‚   â””â”€â”€ exceptions.py              # Custom exceptions
â”‚
â”œâ”€â”€ app.py                          # FastAPI application
â”œâ”€â”€ main.py                         # Entry point
â”œâ”€â”€ config.py                       # Configuration management
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ .env.example                    # Environment template
â””â”€â”€ AI4Bharat_STT (1).ipynb        # Original notebook (reference)
```

### Directory Details

| Directory | Purpose |
|-----------|---------|
| `models/` | STT model implementations (was `models.py/` - now correctly named) |
| `services/` | Routing, audio processing, and metrics |
| `utils/` | Logging, exception handling, helpers |
| `models_cache/` | Cached model weights (auto-created) |
| `temp_audio/` | Temporary audio files (auto-cleaned) |
| `logs/` | Application logs (auto-created) |

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone or navigate to project
cd REALTIME_STT

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download Indic-Speech (for AI4Bharat)
git clone https://github.com/AI4Bharat/Indic-Speech.git
```

### 2. Configuration

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your settings (optional)
# Most settings have sensible defaults
```

### 3. Run the Service

```bash
# Development mode (with auto-reload)
python main.py --reload

# Production mode
python main.py --host 0.0.0.0 --port 8000 --workers 4

# Or using uvicorn directly
uvicorn app:app --host 0.0.0.0 --port 8000 --workers 4
```

### 4. Access API

- **API Docs**: http://localhost:8000/docs
- **OpenAPI Schema**: http://localhost:8000/openapi.json
- **Health Check**: http://localhost:8000/health

---

## ğŸ“¡ API Endpoints

### 1. Health Check
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "service": "Unified STT",
  "version": "1.0.0"
}
```

### 2. Get Supported Languages
```http
GET /languages
```

**Response:**
```json
{
  "languages": {
    "english": ["en"],
    "indic": ["hi", "bn", "ta", "te", "mr", ...],
    "total": 23
  }
}
```

### 3. Transcribe Single Audio
```http
POST /transcribe
Content-Type: multipart/form-data

file: <audio_file>
language: hi
target_text: (optional) "reference text for WER"
decoding: ctc  (optional, for AI4Bharat: ctc or rnnt)
```

**Response:**
```json
{
  "transcript": "à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥‹?",
  "language": "hi",
  "metrics": {
    "confidence": 0.8745,
    "rtf": 0.3456,
    "duration_seconds": 2.50,
    "inference_time_ms": 858.23,
    "wer": 0.1234,
    "accuracy": 87.66
  }
}
```

### 4. Batch Transcribe
```http
POST /batch-transcribe
Content-Type: multipart/form-data

files: <file1>, <file2>, ...
language: hi
```

**Response:**
```json
{
  "results": [
    {
      "filename": "audio1.wav",
      "transcript": "à¤¨à¤®à¤¸à¥à¤¤à¥‡",
      "confidence": 0.89,
      "rtf": 0.34
    },
    {
      "filename": "audio2.wav",
      "transcript": "à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥‹?",
      "confidence": 0.87,
      "rtf": 0.31
    }
  ],
  "total": 2,
  "successful": 2
}
```

---

## ğŸ”§ Configuration Options

### config.py Variables

```python
# Device
DEVICE = "cuda" or "cpu"  # Auto-detected

# Audio
SAMPLE_RATE = 16000  # Hz
MAX_AUDIO_LENGTH = 300  # seconds
MIN_AUDIO_LENGTH = 0.5  # seconds

# Inference
WHISPER_BEAM_SIZE = 5
WHISPER_MODEL_SIZE = "small"  # tiny, base, small, medium, large

# API
API_HOST = "0.0.0.0"
API_PORT = 8000
API_WORKERS = 4
```

### Environment Variables (.env)

```bash
# Required for HuggingFace models
HF_TOKEN=your_token_here

# Optional overrides
WHISPER_MODEL_SIZE=base
COMPUTE_TYPE=float16  # int8, float16, float32
LOG_LEVEL=DEBUG
```

---

## ğŸ“Š Metrics Explained

### RTF (Real-Time Factor)
```
RTF = Inference Time / Audio Duration

RTF < 1.0  â†’ Faster than real-time âœ…
RTF = 1.0  â†’ Real-time
RTF > 1.0  â†’ Slower than real-time
```

### Confidence
- **Range**: 0.0 - 1.0
- Higher = More confident
- AI4Bharat: Token-level average confidence
- Whisper: Language detection confidence

### WER (Word Error Rate)
```
WER = (S + D + I) / N

S = Substitutions
D = Deletions
I = Insertions
N = Number of words in reference

Lower WER = Better accuracy
```

### Accuracy
```
Accuracy = (1 - WER) Ã— 100%
```

---

## ğŸƒ Usage Examples

### Python Client Example

```python
import requests

# Single audio transcription
files = {"file": open("audio.wav", "rb")}
data = {
    "language": "hi",
    "target_text": "à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥‹?"  # Optional
}

response = requests.post(
    "http://localhost:8000/transcribe",
    files=files,
    data=data
)

result = response.json()
print(f"Transcript: {result['transcript']}")
print(f"Confidence: {result['metrics']['confidence']}")
print(f"Accuracy: {result['metrics'].get('accuracy', 'N/A')}%")
```

### cURL Example

```bash
curl -X POST "http://localhost:8000/transcribe" \
  -H "accept: application/json" \
  -F "file=@audio.wav" \
  -F "language=hi" \
  -F "target_text=à¤¨à¤®à¤¸à¥à¤¤à¥‡"
```

### JavaScript/Fetch Example

```javascript
const formData = new FormData();
formData.append("file", audioFile);
formData.append("language", "hi");
formData.append("target_text", "à¤¨à¤®à¤¸à¥à¤¤à¥‡");

const response = await fetch("http://localhost:8000/transcribe", {
  method: "POST",
  body: formData
});

const result = await response.json();
console.log("Transcript:", result.transcript);
console.log("Metrics:", result.metrics);
```

---

## ğŸ› ï¸ Development & Testing

### Run Tests

```bash
# Install test dependencies
pip install pytest pytest-asyncio

# Run tests
pytest tests/

# With coverage
pytest --cov=. tests/
```

### Code Quality

```bash
# Format code
black .

# Lint
flake8 .

# Type checking
mypy .
```

### Logging

All logs are saved to `logs/` directory in JSON format for structured analysis.

```bash
# Enable debug logging
LOG_LEVEL=DEBUG python main.py
```

---

## âš¡ Performance Optimization

### For Production

1. **Use GPU Inference**
   ```bash
   # NVIDIA GPU required
   pip install torch torchcuda  # Adjust based on your CUDA version
   DEVICE=cuda python main.py
   ```

2. **Model Caching**
   - Models are auto-cached in `models_cache/`
   - First run downloads models (slow)
   - Subsequent runs use cache (fast)

3. **Batch Processing**
   - Use `/batch-transcribe` for multiple files
   - Processes parallelizable operations

4. **Worker Scaling**
   ```bash
   python main.py --workers 8  # Increase based on CPU cores
   ```

### Benchmarks (on RTX 3090)

| Language | Model | RTF | Duration | Speed |
|----------|-------|-----|----------|-------|
| English | Whisper-Small | 0.15 | 60s | 4x real-time |
| Hindi | AI4Bharat CTC | 0.45 | 30s | 2.2x real-time |
| Bengali | AI4Bharat CTC | 0.42 | 30s | 2.4x real-time |

---

## ğŸ› Troubleshooting

### Issue: CUDA Out of Memory
```bash
# Reduce model size
WHISPER_MODEL_SIZE=tiny python main.py

# Or use CPU
# Whisper detects automatically, add to .env:
# DEVICE=cpu
```

### Issue: Model Download Fails
```bash
# Set HuggingFace token
export HF_TOKEN=your_token_here
python main.py

# Or in .env file
HF_TOKEN=your_token_here
```

### Issue: Poor Hinglish Recognition
- Ensure audio quality is good
- Hinglish detection uses word ratio heuristic
- Use `initial_prompt` for better accuracy

---

## ğŸ“ Logging

### Log Levels
- **DEBUG**: Detailed information for debugging
- **INFO**: General information (default)
- **WARNING**: Warning messages about potential issues
- **ERROR**: Error information

### Structured Logging

All logs include:
```json
{
  "timestamp": "2024-01-15T10:30:45.123456",
  "level": "INFO",
  "logger": "services.router",
  "message": "Transcription completed",
  "module": "router",
  "function": "transcribe",
  "line": 245
}
```

---

## ğŸ” Security Considerations

1. **File Upload Limits**
   - Default max: 50 MB
   - Configure in `config.py`: `FILE_UPLOAD_MAX_SIZE`

2. **Rate Limiting**
   - Not implemented by default
   - Add with FastAPI middleware for production

3. **Authentication**
   - Add FastAPI security (API key, OAuth2) as needed

4. **Input Validation**
   - Language codes validated against allowed list
   - File formats validated by audio processing

---

## ğŸ“¦ Dependencies

### Core
- **FastAPI**: Modern web framework
- **Uvicorn**: ASGI server
- **PyTorch**: Deep learning
- **Transformers**: Model implementations

### STT
- **faster-whisper**: Fast Whisper inference
- **librosa**: Audio processing
- **soundfile**: Audio I/O

### Utilities
- **jiwer**: WER calculation
- **python-dotenv**: Environment management
- **Gradio** (optional): Web UI

---

## ğŸ“„ License

This project uses:
- Whisper: OpenAI's open-source model
- AI4Bharat: Open-source Indic ASR models

Refer to respective licenses for usage terms.

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Follow PEP 8 style guide
2. Add tests for new features
3. Update documentation
4. Use semantic commit messages

---

## ğŸ“ Support

For issues or questions:
1. Check troubleshooting section
2. Review logs in `logs/` directory
3. File an issue on GitHub

---

## ğŸ™ Acknowledgments

- **OpenAI** for Whisper
- **AI4Bharat** for Indic Conformer and Indic-Speech models
- **FastAPI** community for the excellent framework

---

**Last Updated**: January 2024  
**Version**: 1.0.0  
**Status**: Production Ready âœ…
#
>>>>>>> e956736d109726caf94444e3df35649d7889edcf
