# üéôÔ∏è Unified STT Service
## Production-Grade Speech-to-Text for English & 22+ Indian Languages

A high-performance, scalable speech-to-text service supporting:
- **English**: OpenAI Whisper (faster-whisper)
- **Indian Languages**: AI4Bharat Indic Conformer (22+ languages)
- **Hinglish**: Automatic mix-language support

---

## üìã Features

‚úÖ **Multi-Language Support**
- English (via Whisper)
- 22+ Indian Languages: Hindi, Bengali, Tamil, Telugu, Marathi, Gujarati, Kannada, Malayalam, Punjabi, Urdu, Assamese, Odia, Sanskrit, Sindhi, Nepali, Sinhala, Konkani, Maithili, Dogri, Manipuri, Bodo, Santali

‚úÖ **Intelligent Routing**
- Automatic language detection
- Hinglish (Hindi-English mix) support
- Optimized model selection per language

‚úÖ **Production Ready**
- Structured logging (JSON format)
- Comprehensive error handling
- Graceful shutdown
- Request tracking
- Background task cleanup

‚úÖ **Performance Metrics**
- Real-Time Factor (RTF) calculation
- Confidence scoring
- Word Error Rate (WER) if ground truth provided
- Accuracy reporting

‚úÖ **Batch Processing**
- Multi-file transcription
- Parallel processing support
- Error recovery per file

---

## üèóÔ∏è Project Structure

```
REALTIME_STT/
‚îú‚îÄ‚îÄ models/                          # STT Model implementations
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base.py                     # Abstract base class
‚îÇ   ‚îú‚îÄ‚îÄ ai4bharat_stt.py           # AI4Bharat implementation
‚îÇ   ‚îî‚îÄ‚îÄ whisper_stt.py             # Whisper implementation
‚îÇ
‚îú‚îÄ‚îÄ services/                        # Business logic layer
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ router.py                  # Unified routing logic
‚îÇ   ‚îú‚îÄ‚îÄ audio_utils.py             # Audio processing utilities
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py                 # Metrics calculation
‚îÇ
‚îú‚îÄ‚îÄ utils/                           # Utility modules
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ logger.py                  # Structured logging
‚îÇ   ‚îî‚îÄ‚îÄ exceptions.py              # Custom exceptions
‚îÇ
‚îú‚îÄ‚îÄ app.py                          # FastAPI application
‚îú‚îÄ‚îÄ main.py                         # Entry point
‚îú‚îÄ‚îÄ config.py                       # Configuration management
‚îú‚îÄ‚îÄ requirements.txt                # Dependencies
‚îú‚îÄ‚îÄ .env.example                    # Environment template
‚îî‚îÄ‚îÄ AI4Bharat_STT (1).ipynb        # Original notebook (reference)
```

### Directory Details

| Directory | Purpose |
|-----------|---------|
| `models/` | STT model implementations (was `models.py/` - now correctly named) |
| `services/` | Routing, audio processing, and metrics |
| `utils/` | Logging, exception handling, helpers |
| `models_cache/` | Cached model weights (auto-created) |
| `temp_audio/` | Temporary audio files (auto-cleaned) |
| `logs/` | Application logs (auto-created) |

---

## üöÄ Quick Start

### 1. Installation

```bash
# Clone or navigate to project
cd REALTIME_STT

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download Indic-Speech (for AI4Bharat)
git clone https://github.com/AI4Bharat/Indic-Speech.git
```

### 2. Configuration

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your settings (optional)
# Most settings have sensible defaults
```

### 3. Run the Service

```bash
# Development mode (with auto-reload)
python main.py --reload

# Production mode
python main.py --host 0.0.0.0 --port 8000 --workers 4

# Or using uvicorn directly
uvicorn app:app --host 0.0.0.0 --port 8000 --workers 4
```

### 4. Access API

- **API Docs**: http://localhost:8000/docs
- **OpenAPI Schema**: http://localhost:8000/openapi.json
- **Health Check**: http://localhost:8000/health

---

## üì° API Endpoints

### 1. Health Check
```http
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "service": "Unified STT",
  "version": "1.0.0"
}
```

### 2. Get Supported Languages
```http
GET /languages
```

**Response:**
```json
{
  "languages": {
    "english": ["en"],
    "indic": ["hi", "bn", "ta", "te", "mr", ...],
    "total": 23
  }
}
```

### 3. Transcribe Single Audio
```http
POST /transcribe
Content-Type: multipart/form-data

file: <audio_file>
language: hi
target_text: (optional) "reference text for WER"
decoding: ctc  (optional, for AI4Bharat: ctc or rnnt)
```

**Response:**
```json
{
  "transcript": "‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•ã?",
  "language": "hi",
  "metrics": {
    "confidence": 0.8745,
    "rtf": 0.3456,
    "duration_seconds": 2.50,
    "inference_time_ms": 858.23,
    "wer": 0.1234,
    "accuracy": 87.66
  }
}
```

### 4. Batch Transcribe
```http
POST /batch-transcribe
Content-Type: multipart/form-data

files: <file1>, <file2>, ...
language: hi
```

**Response:**
```json
{
  "results": [
    {
      "filename": "audio1.wav",
      "transcript": "‡§®‡§Æ‡§∏‡•ç‡§§‡•á",
      "confidence": 0.89,
      "rtf": 0.34
    },
    {
      "filename": "audio2.wav",
      "transcript": "‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•ã?",
      "confidence": 0.87,
      "rtf": 0.31
    }
  ],
  "total": 2,
  "successful": 2
}
```

---

## üîß Configuration Options

### config.py Variables

```python
# Device
DEVICE = "cuda" or "cpu"  # Auto-detected

# Audio
SAMPLE_RATE = 16000  # Hz
MAX_AUDIO_LENGTH = 300  # seconds
MIN_AUDIO_LENGTH = 0.5  # seconds

# Inference
WHISPER_BEAM_SIZE = 5
WHISPER_MODEL_SIZE = "small"  # tiny, base, small, medium, large

# API
API_HOST = "0.0.0.0"
API_PORT = 8000
API_WORKERS = 4
```

### Environment Variables (.env)

```bash
# Required for HuggingFace models
HF_TOKEN=your_token_here

# Optional overrides
WHISPER_MODEL_SIZE=base
COMPUTE_TYPE=float16  # int8, float16, float32
LOG_LEVEL=DEBUG
```

---

## üìä Metrics Explained

### RTF (Real-Time Factor)
```
RTF = Inference Time / Audio Duration

RTF < 1.0  ‚Üí Faster than real-time ‚úÖ
RTF = 1.0  ‚Üí Real-time
RTF > 1.0  ‚Üí Slower than real-time
```

### Confidence
- **Range**: 0.0 - 1.0
- Higher = More confident
- AI4Bharat: Token-level average confidence
- Whisper: Language detection confidence

### WER (Word Error Rate)
```
WER = (S + D + I) / N

S = Substitutions
D = Deletions
I = Insertions
N = Number of words in reference

Lower WER = Better accuracy
```

### Accuracy
```
Accuracy = (1 - WER) √ó 100%
```

---

## üèÉ Usage Examples

### Python Client Example

```python
import requests

# Single audio transcription
files = {"file": open("audio.wav", "rb")}
data = {
    "language": "hi",
    "target_text": "‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•ã?"  # Optional
}

response = requests.post(
    "http://localhost:8000/transcribe",
    files=files,
    data=data
)

result = response.json()
print(f"Transcript: {result['transcript']}")
print(f"Confidence: {result['metrics']['confidence']}")
print(f"Accuracy: {result['metrics'].get('accuracy', 'N/A')}%")
```

### cURL Example

```bash
curl -X POST "http://localhost:8000/transcribe" \
  -H "accept: application/json" \
  -F "file=@audio.wav" \
  -F "language=hi" \
  -F "target_text=‡§®‡§Æ‡§∏‡•ç‡§§‡•á"
```

### JavaScript/Fetch Example

```javascript
const formData = new FormData();
formData.append("file", audioFile);
formData.append("language", "hi");
formData.append("target_text", "‡§®‡§Æ‡§∏‡•ç‡§§‡•á");

const response = await fetch("http://localhost:8000/transcribe", {
  method: "POST",
  body: formData
});

const result = await response.json();
console.log("Transcript:", result.transcript);
console.log("Metrics:", result.metrics);
```

---

## üõ†Ô∏è Development & Testing

### Run Tests

```bash
# Install test dependencies
pip install pytest pytest-asyncio

# Run tests
pytest tests/

# With coverage
pytest --cov=. tests/
```

### Code Quality

```bash
# Format code
black .

# Lint
flake8 .

# Type checking
mypy .
```

### Logging

All logs are saved to `logs/` directory in JSON format for structured analysis.

```bash
# Enable debug logging
LOG_LEVEL=DEBUG python main.py
```

---

## ‚ö° Performance Optimization

### For Production

1. **Use GPU Inference**
   ```bash
   # NVIDIA GPU required
   pip install torch torchcuda  # Adjust based on your CUDA version
   DEVICE=cuda python main.py
   ```

2. **Model Caching**
   - Models are auto-cached in `models_cache/`
   - First run downloads models (slow)
   - Subsequent runs use cache (fast)

3. **Batch Processing**
   - Use `/batch-transcribe` for multiple files
   - Processes parallelizable operations

4. **Worker Scaling**
   ```bash
   python main.py --workers 8  # Increase based on CPU cores
   ```

### Benchmarks (on RTX 3090)

| Language | Model | RTF | Duration | Speed |
|----------|-------|-----|----------|-------|
| English | Whisper-Small | 0.15 | 60s | 4x real-time |
| Hindi | AI4Bharat CTC | 0.45 | 30s | 2.2x real-time |
| Bengali | AI4Bharat CTC | 0.42 | 30s | 2.4x real-time |

---

## üêõ Troubleshooting

### Issue: CUDA Out of Memory
```bash
# Reduce model size
WHISPER_MODEL_SIZE=tiny python main.py

# Or use CPU
# Whisper detects automatically, add to .env:
# DEVICE=cpu
```

### Issue: Model Download Fails
```bash
# Set HuggingFace token
export HF_TOKEN=your_token_here
python main.py

# Or in .env file
HF_TOKEN=your_token_here
```

### Issue: Poor Hinglish Recognition
- Ensure audio quality is good
- Hinglish detection uses word ratio heuristic
- Use `initial_prompt` for better accuracy

---

## üìù Logging

### Log Levels
- **DEBUG**: Detailed information for debugging
- **INFO**: General information (default)
- **WARNING**: Warning messages about potential issues
- **ERROR**: Error information

### Structured Logging

All logs include:
```json
{
  "timestamp": "2024-01-15T10:30:45.123456",
  "level": "INFO",
  "logger": "services.router",
  "message": "Transcription completed",
  "module": "router",
  "function": "transcribe",
  "line": 245
}
```

---

## üîê Security Considerations

1. **File Upload Limits**
   - Default max: 50 MB
   - Configure in `config.py`: `FILE_UPLOAD_MAX_SIZE`

2. **Rate Limiting**
   - Not implemented by default
   - Add with FastAPI middleware for production

3. **Authentication**
   - Add FastAPI security (API key, OAuth2) as needed

4. **Input Validation**
   - Language codes validated against allowed list
   - File formats validated by audio processing

---

## üì¶ Dependencies

### Core
- **FastAPI**: Modern web framework
- **Uvicorn**: ASGI server
- **PyTorch**: Deep learning
- **Transformers**: Model implementations

### STT
- **faster-whisper**: Fast Whisper inference
- **librosa**: Audio processing
- **soundfile**: Audio I/O

### Utilities
- **jiwer**: WER calculation
- **python-dotenv**: Environment management
- **Gradio** (optional): Web UI

---

## üìÑ License

This project uses:
- Whisper: OpenAI's open-source model
- AI4Bharat: Open-source Indic ASR models

Refer to respective licenses for usage terms.

---

## ü§ù Contributing

Contributions welcome! Please:
1. Follow PEP 8 style guide
2. Add tests for new features
3. Update documentation
4. Use semantic commit messages

---

## üìû Support

For issues or questions:
1. Check troubleshooting section
2. Review logs in `logs/` directory
3. File an issue on GitHub

---

## üôè Acknowledgments

- **OpenAI** for Whisper
- **AI4Bharat** for Indic Conformer and Indic-Speech models
- **FastAPI** community for the excellent framework

---

**Last Updated**: January 2024  
**Version**: 1.0.0  
**Status**: Production Ready ‚úÖ
#   a i 4 b h a r a t _ s t t  
 